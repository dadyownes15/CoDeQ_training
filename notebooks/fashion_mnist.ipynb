{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f125230d",
   "metadata": {},
   "source": [
    "Load the data\n",
    "\n",
    "Train a simple MLP with and without pruning\n",
    "    1. What structure? We can experiment with it\n",
    "\n",
    "Evaluate the accurcary\n",
    "\n",
    "Use the experiment class to calculate and compare bobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb895840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from src.train import train, evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.networks_fashion_mnist import MLP_small_fashion_mnist\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from src.utils_quantization import attach_weight_quantizers, toggle_quantization\n",
    "from src.quantizer import DeadZoneLDZCompander\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_fashion_transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.2860,), (0.3530,))\n",
    "]\n",
    ")\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=mnist_fashion_transform\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1750aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_sparsity_reg(model, lambda_linear=1e-4, lambda_conv=1e-4):\n",
    "    reg = 0.0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            W = m.weight  # [out, in]\n",
    "            reg = reg + lambda_linear * torch.norm(W, dim=0).sum()\n",
    "            #print(m, reg)\n",
    "            #reg = reg + lambda_linear * torch.norm(W, dim=1).sum()\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            W = m.weight  # [out_c, in_c, kH, kW]\n",
    "            # Flatten each filter to vector then one group per output channel\n",
    "            W_flat = W.view(W.size(0), -1)  # [out_c, in_c*kH*kW]\n",
    "            reg = reg + lambda_conv * torch.norm(W_flat, dim=0).sum()\n",
    "            #reg = reg + lambda_conv * torch.norm(W_flat, dim=1).sum()\n",
    "    return reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c69201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_small_fashion_mnist()\n",
    "total_size = len(training_data)\n",
    "training_percentage = 0.05\n",
    "training_size = round(total_size*training_percentage)\n",
    "\n",
    "remaining, _  = random_split(training_data,[training_size, total_size - training_size]) \n",
    "\n",
    "val_split_percentage = 0.10\n",
    "val_size = round(val_split_percentage*training_size)\n",
    "\n",
    "train_dataset, val_dataset = random_split(remaining, [len(remaining) - val_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset)\n",
    "\n",
    "LR_BASE = 0.001\n",
    "LR_DZ = 0.001\n",
    "LR_BIT = 0.001\n",
    "\n",
    "WD = 0.0\n",
    "WD_DZ = 2.5 #2.5\n",
    "WD_BIT = 0.0 #2.5\n",
    "REG_STRUCTURED = 0.1\n",
    "QUANT_ARGS = {\"fixed_bit_val\": 8, \"init_deadzone_logit\": 3.0, \"max_bits\": 8, \"learnable_deadzone\": True, \"learnable_bit\": False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c616d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached weight quantizer to layer: net.0\n",
      "Attached weight quantizer to layer: net.2\n",
      "Attached weight quantizer to layer: net.4\n",
      "Epoch [1/1] | Train Loss: 1.4696, Train Acc: 55.04% | Val Loss: 0.8230, Val Acc: 72.33%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 1\n",
    "\n",
    "attach_weight_quantizers(model=model,\n",
    "                         exclude_layers=[],\n",
    "                         quantizer=DeadZoneLDZCompander,\n",
    "                         quantizer_kwargs=QUANT_ARGS,\n",
    "                         enabled=True\n",
    "                         )\n",
    "\n",
    "base_params = []\n",
    "dz_params = []\n",
    "bit_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'logit_dz' in name:\n",
    "        dz_params.append(param)\n",
    "    elif 'logit_bit' in name:\n",
    "        bit_params.append(param)\n",
    "    else:\n",
    "        base_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': base_params, 'lr': LR_BASE, 'weight_decay': WD},\n",
    "    {'params': dz_params, 'lr': LR_DZ, 'weight_decay': WD_DZ},\n",
    "    {'params': bit_params, 'lr': LR_BIT, 'weight_decay': WD_BIT},\n",
    "])\n",
    "\n",
    "toggle_quantization(model=model,enabled=True)\n",
    "\n",
    "\n",
    "train(epochs=epochs,model=model,optimizer=optimizer,criterion=criterion,train_loader=train_loader,val_loader=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_small_fashion_mnist(\n",
      "  (net): Sequential(\n",
      "    (0): ParametrizedLinear(\n",
      "      in_features=784, out_features=256, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): FakeQuantParametrization(\n",
      "            (quantizer): DeadZoneLDZCompander()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): ParametrizedLinear(\n",
      "      in_features=256, out_features=128, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): FakeQuantParametrization(\n",
      "            (quantizer): DeadZoneLDZCompander()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): ParametrizedLinear(\n",
      "      in_features=128, out_features=10, bias=True\n",
      "      (parametrizations): ModuleDict(\n",
      "        (weight): ParametrizationList(\n",
      "          (0): FakeQuantParametrization(\n",
      "            (quantizer): DeadZoneLDZCompander()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.bobs_calculator import compare_model \n",
    "\n",
    "compare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f0097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c396a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972e32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
