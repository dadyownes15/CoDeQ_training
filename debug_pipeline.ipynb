{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoDeQ Debug Pipeline Validation\n",
    "\n",
    "Trains ResNet-20 on CIFAR-10 for 20 epochs with coupled group lasso sparsity.\n",
    "Validates the full pipeline: quantization-aware training, structured sparsity, and compression evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.resnet import resnet20\n",
    "from src.quantizer import DeadZoneLDZCompander\n",
    "from src.utils_quantization import attach_weight_quantizers, toggle_quantization\n",
    "from experiments.evaluator import Evaluator\n",
    "from experiments.sparsity import coupled_group_lasso\n",
    "from experiments.schedules import linear_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unstructured_sparsity(model: nn.Module) -> float:\n",
    "    \"\"\"Fraction of quantized weights that are exactly zero.\"\"\"\n",
    "    total = 0\n",
    "    zeros = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) and hasattr(m, 'parametrizations'):\n",
    "            w = m.weight.detach().cpu()\n",
    "            total += w.numel()\n",
    "            zeros += (w == 0).sum().item()\n",
    "    return zeros / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_structured_sparsity(model: nn.Module) -> float:\n",
    "    \"\"\"Fraction of output channels that are entirely dead across quantized conv layers.\"\"\"\n",
    "    total_channels = 0\n",
    "    dead_channels = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) and hasattr(m, 'parametrizations'):\n",
    "            w = m.weight.detach().cpu()\n",
    "            c_out = w.shape[0]\n",
    "            alive = w.flatten(1).norm(p=2, dim=1) > 0\n",
    "            total_channels += c_out\n",
    "            dead_channels += (c_out - alive.sum().item())\n",
    "    return dead_channels / total_channels if total_channels > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_avg_bitwidth(model: nn.Module) -> float:\n",
    "    \"\"\"Mean learned bitwidth across all quantized layers.\"\"\"\n",
    "    bitwidths = []\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'parametrizations') and hasattr(m.parametrizations, 'weight'):\n",
    "            fq = m.parametrizations.weight[0]\n",
    "            bitwidths.append(fq.quantizer.get_bitwidth().item())\n",
    "    return sum(bitwidths) / len(bitwidths) if bitwidths else 0.0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, test_loader, device: str) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return correct / total, total_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikkeldahl/CoDeQ/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=2, pin_memory=(DEVICE == \"cuda\"))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=(DEVICE == \"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model + Quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached weight quantizer to layer: layer1.0.conv2\n",
      "Attached weight quantizer to layer: layer1.1.conv2\n",
      "Attached weight quantizer to layer: layer1.2.conv2\n",
      "Attached weight quantizer to layer: layer2.0.conv2\n",
      "Attached weight quantizer to layer: layer2.1.conv2\n",
      "Attached weight quantizer to layer: layer2.2.conv2\n",
      "Attached weight quantizer to layer: layer3.0.conv2\n",
      "Attached weight quantizer to layer: layer3.1.conv2\n",
      "Attached weight quantizer to layer: layer3.2.conv2\n"
     ]
    }
   ],
   "source": [
    "model = resnet20()\n",
    "\n",
    "# Baseline snapshot (before quantization)\n",
    "evaluator = Evaluator(model, input_size=(3, 32, 32))\n",
    "\n",
    "# Attach quantizers\n",
    "attach_weight_quantizers(\n",
    "    model=model,\n",
    "    exclude_layers=['conv1', 'bn', 'linear'],\n",
    "    quantizer=DeadZoneLDZCompander,\n",
    "    quantizer_kwargs={\n",
    "        'max_bits': 8,\n",
    "        'init_bit_logit': 3.0,\n",
    "        'init_deadzone_logit': 3.0,\n",
    "        'learnable_bit': True,\n",
    "        'learnable_deadzone': True,\n",
    "    },\n",
    "    enabled=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Sparsity loss\n",
    "sparsity_fn = coupled_group_lasso(model)\n",
    "sparsity_schedule = linear_warmup(delay=5, ramp=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base params: 59, DZ params: 9, Bit params: 9\n"
     ]
    }
   ],
   "source": [
    "base_params, dz_params, bit_params = [], [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'logit_dz' in name:\n",
    "        dz_params.append(param)\n",
    "    elif 'logit_bit' in name:\n",
    "        bit_params.append(param)\n",
    "    else:\n",
    "        base_params.append(param)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': base_params, 'lr': 1e-3, 'weight_decay': 0.0},\n",
    "    {'params': dz_params, 'lr': 1e-3, 'weight_decay': 0.01},\n",
    "    {'params': bit_params, 'lr': 1e-3, 'weight_decay': 0.01},\n",
    "])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Base params: {len(base_params)}, DZ params: {len(dz_params)}, Bit params: {len(bit_params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch |     Loss |     Acc |   Unstr |  Struct |  Bits\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     16\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoDeQ/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoDeQ/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1509\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent_workers:\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1510\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1512\u001b[39m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[32m   1513\u001b[39m \n\u001b[32m   1514\u001b[39m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoDeQ/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1671\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1666\u001b[39m         \u001b[38;5;28mself\u001b[39m._mark_worker_as_unavailable(worker_id, shutdown=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers:\n\u001b[32m   1668\u001b[39m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[32m   1670\u001b[39m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1671\u001b[39m     \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_queues:\n\u001b[32m   1673\u001b[39m     q.cancel_join_thread()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/process.py:149\u001b[39m, in \u001b[36mBaseProcess.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_pid == os.getpid(), \u001b[33m'\u001b[39m\u001b[33mcan only join a child process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mcan only join a started process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_popen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     _children.discard(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/popen_fork.py:40\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/connection.py:1135\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1132\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'epoch': [], 'accuracy': [], 'loss': [],\n",
    "    'unstr_sparsity': [], 'struct_sparsity': [],\n",
    "    'avg_bits': [], 'coeff': [],\n",
    "}\n",
    "\n",
    "print(f\"{'Epoch':>7} | {'Loss':>8} | {'Acc':>7} | {'Unstr':>7} | {'Struct':>7} | {'Bits':>5}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    toggle_quantization(model, enabled=True)\n",
    "\n",
    "    coeff = sparsity_schedule(epoch, EPOCHS)\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        ce_loss = criterion(outputs, labels)\n",
    "        sp_loss = sparsity_fn(model)\n",
    "        total_loss = ce_loss + coeff * sp_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate\n",
    "    test_acc, test_loss = evaluate(model, test_loader, DEVICE)\n",
    "    unstr = compute_unstructured_sparsity(model)\n",
    "    struct = compute_structured_sparsity(model)\n",
    "    avg_bits = compute_avg_bitwidth(model)\n",
    "    avg_loss = running_loss / num_batches\n",
    "\n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['accuracy'].append(test_acc)\n",
    "    history['loss'].append(avg_loss)\n",
    "    history['unstr_sparsity'].append(unstr)\n",
    "    history['struct_sparsity'].append(struct)\n",
    "    history['avg_bits'].append(avg_bits)\n",
    "    history['coeff'].append(coeff)\n",
    "\n",
    "    print(\n",
    "        f\"  {epoch+1:>3}/{EPOCHS} | \"\n",
    "        f\"{avg_loss:>8.4f} | \"\n",
    "        f\"{test_acc*100:>5.1f}% | \"\n",
    "        f\"{unstr*100:>5.1f}% | \"\n",
    "        f\"{struct*100:>5.1f}% | \"\n",
    "        f\"{avg_bits:>5.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Compression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()  # Evaluator needs model on CPU for spatial dim profiling\n",
    "result = evaluator.evaluate(model)\n",
    "\n",
    "print(f\"Structured BOPs compression:   {result.structured_bops_ratio:.2f}x\")\n",
    "print(f\"Unstructured BOPs compression: {result.unstructured_bops_ratio:.2f}x\")\n",
    "print(f\"Structured MAC compression:    {result.structured_mac_ratio:.2f}x\")\n",
    "print(f\"Unstructured MAC compression:  {result.unstructured_mac_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Accuracy Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(history['epoch'], [a * 100 for a in history['accuracy']], 'b-o', markersize=4)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Test Accuracy Over Training')\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Sparsity Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax1.plot(history['epoch'], [s * 100 for s in history['unstr_sparsity']],\n",
    "         'r-o', markersize=4, label='Unstructured Sparsity')\n",
    "ax1.plot(history['epoch'], [s * 100 for s in history['struct_sparsity']],\n",
    "         'b-s', markersize=4, label='Structured Sparsity')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Sparsity (%)')\n",
    "ax1.set_title('Sparsity Over Training')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history['epoch'], history['coeff'], 'g--', alpha=0.6, label='Sparsity Coeff')\n",
    "ax2.set_ylabel('Sparsity Loss Coefficient', color='green')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Channel Liveness Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluator.plot_channel_liveness(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: Per-Layer Channel Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "alive_counts = []\n",
    "dead_counts = []\n",
    "bitwidths = []\n",
    "\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m, nn.Conv2d) and hasattr(m, 'parametrizations'):\n",
    "        w = m.weight.detach()\n",
    "        c_out = w.shape[0]\n",
    "        alive = (w.flatten(1).norm(p=2, dim=1) > 0).sum().item()\n",
    "        dead = c_out - alive\n",
    "        bw = m.parametrizations.weight[0].quantizer.get_bitwidth().item()\n",
    "\n",
    "        layer_names.append(name)\n",
    "        alive_counts.append(alive)\n",
    "        dead_counts.append(dead)\n",
    "        bitwidths.append(bw)\n",
    "\n",
    "x = np.arange(len(layer_names))\n",
    "fig, ax = plt.subplots(figsize=(max(8, len(layer_names) * 0.8), 6))\n",
    "ax.bar(x, alive_counts, label='Alive', color='steelblue')\n",
    "ax.bar(x, dead_counts, bottom=alive_counts, label='Dead', color='lightgray')\n",
    "\n",
    "for i, bw in enumerate(bitwidths):\n",
    "    total = alive_counts[i] + dead_counts[i]\n",
    "    ax.text(i, total + 0.5, f'{bw:.0f}b', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(layer_names, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel('Channels')\n",
    "ax.set_title('Per-Layer Channel Status (annotated with learned bitwidth)')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
